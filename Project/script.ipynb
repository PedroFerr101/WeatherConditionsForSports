{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Sistemas Autónomos\n",
    "#### \"Monitorização de parâmetros ambientais para a prática de desporto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import config\n",
    "import firebase_admin\n",
    "from firebase_admin import auth, credentials, firestore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Database from Cloud Firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the key\n",
    "cred = credentials.Certificate(config.firestore_key)\n",
    "\n",
    "# Initialize/Get the app\n",
    "try:\n",
    "    app = firebase_admin.initialize_app(cred)\n",
    "except:\n",
    "    app = firebase_admin.get_app()\n",
    "\n",
    "# Get the database\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the 'WM', 'UV' and 'AQ' collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ------- Open Weather Map -------\n",
    "\n",
    "dataWM = pd.DataFrame(columns=['year','month','day','hours','minutes','hours.minutes','feels_like','general_weather','humidity','pressure','temp_min','temp_max','wind_speed'])\n",
    "\n",
    "wm_ref = db.collection(u'WM')\n",
    "docsWM = wm_ref.stream()\n",
    "\n",
    "# general_weather tem que se meter em integer, mas temos que decidir se continuamos a extrair o \"description\" ou o \"main\" no momento do request\n",
    "\n",
    "for doc in docsWM:\n",
    "    params = doc.to_dict()\n",
    "    datetime = re.split(r'[-T:]',doc.id)\n",
    "    perc_minutes = int(datetime[3])+(0.0167*int(datetime[4])) # precisava de ter os minutos adaptados para a unidade da hora\n",
    "    dataWM = dataWM.append({'year':int(datetime[0]),'month':int(datetime[1]) ,'day':int(datetime[2]), 'hours':int(datetime[3]), 'minutes': int(datetime[4]), 'hours.minutes': perc_minutes, 'feels_like': float(params['feels_like'])-273.15, 'general_weather':params['general_weather'], 'humidity': int(params['humidity']), 'pressure': int(params['pressure']), 'temp_min': float(params['temp_min'])-273.15, 'temp_max': float(params['temp_max'])-273.15, 'wind_speed': float(params['wind_speed'])}, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------- Open UV -------\n",
    "\n",
    "dataUV = pd.DataFrame(columns=['uv','uv_time','uv_max','uv_max_time','st1','st2','st3','st4','st5','st6'])\n",
    "\n",
    "uv_ref = db.collection(u'UV')\n",
    "docsUV = uv_ref.stream()\n",
    "\n",
    "for doc in docsUV:\n",
    "    params = doc.to_dict()\n",
    "    dataUV = dataUV.append({'uv': float(params['uv']), 'uv_time': params['uv_time'], 'uv_max': float(params['uv_max']), 'uv_max_time': params['uv_max_time'], 'st1': params['st1'], 'st2': params['st2'], 'st3': params['st3'], 'st4': params['st4'], 'st5': params['st5'], 'st6': params['st6']}, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------- Open AQ -------\n",
    "\n",
    "dataAQ = pd.DataFrame(columns=['no2','o3','pm10'])\n",
    "\n",
    "aq_ref = db.collection(u'AQ')\n",
    "docsAQ = aq_ref.stream()\n",
    "\n",
    "for doc in docsAQ:\n",
    "    params = doc.to_dict()\n",
    "    dataAQ = dataAQ.append({'no2': int(params['no2']), 'o3': int(params['o3']), 'pm10': int(params['pm10'])}, ignore_index=True)\n",
    "\n",
    "\n",
    "data = pd.concat([dataWM, dataUV, dataAQ], axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum(axis=0))\n",
    "sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"GnBu_d\")\n",
    "sns.set_style('whitegrid')\n",
    "#sns.jointplot(x='hours.minutes', y='feels_like', data=data)\n",
    "#sns.jointplot(x='hours.minutes', y='feels_like', data=data, kind='kde')\n",
    "sns.jointplot(x='hours.minutes', y='feels_like', data=data, kind='hex')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    feels_like_history = data[['feels_like']]\n",
    "    return feels_like_history\n",
    "\n",
    "\n",
    "def normalize_data(df):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df = scaler.fit_transform(df)\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def plotDataset(data, predictions):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(data)), data['feels_like'], color='blue', label='Confirmed')\n",
    "    plt.plot(range(len(data)-1, len(data)+len(predictions)-1),\n",
    "             predictions, color='red', label='Prediction')\n",
    "    plt.title('Weather')\n",
    "    plt.ylim(0,30) \n",
    "    plt.ylabel('Feels_like')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_supervised(df, timesteps):\n",
    "    data = df.values  # array de arrays com os valores\n",
    "    X, y = list(), list()\n",
    "    dataset_size = len(data)  # nr de linhas\n",
    "    for curr_pos in range(dataset_size):\n",
    "        input_index = curr_pos+timesteps\n",
    "        label_index = input_index+1\n",
    "        if label_index < dataset_size:\n",
    "            X.append(data[curr_pos:input_index, :])\n",
    "            y.append(data[input_index:label_index, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n",
    "\n",
    "def build_model(timesteps, features, dropout_rate=0.5):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True,\n",
    "                                   input_shape=(timesteps, features)))\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        128, return_sequences=True, dropout=dropout_rate))\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        256, return_sequences=False, dropout=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='tanh'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(features, activation='linear'))\n",
    "    model.compile(\n",
    "        loss=rmse,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['accuracy','mae',rmse])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def forecast(model, df, timesteps, multisteps, scaler):\n",
    "    input_seq = df[-timesteps:].values\n",
    "    inp = input_seq\n",
    "    predictions = list()\n",
    "    for step in range(1, multisteps+1):\n",
    "        inp = inp.reshape(1, timesteps, 1)\n",
    "        yhat = model.predict(inp, verbose=1)\n",
    "        yhat_inversed = scaler.inverse_transform(yhat)\n",
    "        predictions.append(yhat_inversed[0][0])\n",
    "        inp = np.append(inp[0], yhat)\n",
    "        inp = inp[-timesteps:]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "multistep = 7\n",
    "features = 1\n",
    "batch_size = 16\n",
    "\n",
    "df_prepared = prepare_data(data)\n",
    "print(df_prepared)\n",
    "df_to_plot = df_prepared.copy()\n",
    "\n",
    "scaler = normalize_data(df_prepared)\n",
    "X, y = to_supervised(df_prepared, timesteps)\n",
    "model = build_model(timesteps, features)\n",
    "model.fit(X, y, shuffle=False, epochs=50, verbose=1, batch_size=batch_size)\n",
    "\n",
    "predictions = forecast(model, df_prepared, timesteps, multistep, scaler)\n",
    "plotDataset(df_to_plot, predictions)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('Sensorization': pipenv)",
   "language": "python",
   "name": "python37664bitsensorizationpipenvb08871551dee4fca81bab2404db1909d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
